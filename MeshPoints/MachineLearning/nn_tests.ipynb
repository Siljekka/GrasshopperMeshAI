{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf",
   "display_name": "Python 3.7.4 64-bit ('masterMLvenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, metrics\n",
    "from tensorflow import feature_column\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sklearn.preprocessing as skpp\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/6-gon-lc-04-mesh-dataset-new.csv')\n",
    "\n",
    "# 2. Separate based on internal nodes added. We choose 2 as it has the highest incidence.\n",
    "#    -> Remove empty columns with dropna.\n",
    "#    -> Remove target_edge_length column (experiment)\n",
    "df = df[df.internal_nodes == 2.0].dropna(axis=1, how='all')\n",
    "dataset = df[df.target_edge_length == 0.4]\n",
    "dataset.drop(['target_edge_length', 'internal_nodes'], axis=1)\n",
    "\n",
    "dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-1\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 3000\n",
    "POLYGON_SIZE = 6\n",
    "\n",
    "tmp_polygons = pd.read_csv(\"data/6-gon-lc-04-mesh-dataset-new.csv\")\n",
    "polygons = tmp_polygons.copy()\n",
    "\n",
    "\n",
    "polygons = sklearn.utils.shuffle(polygons)\n",
    "# Split dataset into 85/15 training/test\n",
    "# Later split training into 70/15 train/validation\n",
    "polygon_train = polygons.sample(frac=0.85, random_state=0)\n",
    "polygon_test = polygons.drop(polygon_train.index)\n",
    "\n",
    "train_features = polygon_train.copy()\n",
    "train_labels = train_features.pop('internal_nodes')\n",
    "\n",
    "test_features = polygon_test.copy()\n",
    "test_labels = test_features.pop('internal_nodes')\n",
    "\n",
    "polygon_model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(13,)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(24, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(24, activation='relu')),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "polygon_model.summary()\n",
    "polygon_model.compile(loss = tf.losses.MeanAbsoluteError(),\n",
    "                      optimizer=tf.optimizers.Adam(\n",
    "                          learning_rate=LEARNING_RATE, decay=WEIGHT_DECAY\n",
    "                      ),\n",
    "                      )\n",
    "\n",
    "# tensorboard support\n",
    "# logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "# %tensorboard --logdir logs\n",
    "\n",
    "history = polygon_model.fit(train_features,\n",
    "                            train_labels,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_split=0.18,\n",
    "                            verbose=1,\n",
    "                            #callbacks=[tensorboard_callback]\n",
    "                            )\n",
    "\n",
    "# Evaluate the model\n",
    "train_acc = polygon_model.evaluate(train_features, train_labels, verbose=0)\n",
    "test_acc = polygon_model.evaluate(test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_model.save('model/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_polygons = pd.read_csv(\"data/6-gon-mesh-dataset.csv\")\n",
    "polygons = tmp_polygons.copy()\n",
    "\n",
    "# Pre-processing of labels\n",
    "scaler = skpp.MinMaxScaler()\n",
    "polygons['internal_nodes'] = scaler.fit_transform(np.array(tmp_polygons['internal_nodes']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some examples\n",
    "import pre_processing as pp\n",
    "model = tf.keras.models.load_model('model/saved_model')\n",
    "sample_triangle = pp.procrustes(pp.create_random_ngon(6))\n",
    "\n",
    "\n",
    "\n",
    "prediction_input = np.append(sample_triangle['transformed_contour'], 0.4)\n",
    "pp.plot_polygon(sample_triangle['transformed_contour'])\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.show()\n",
    "\n",
    "print(prediction_input.shape)\n",
    "\n",
    "internal_node_prediction = model.predict(prediction_input.reshape(-1, 13))  # target_edge_length = 0.4\n",
    "import gmsh\n",
    "gmsh.initialize()\n",
    "actual_value = pp.mesh_contour(sample_triangle['transformed_contour'], 0.4)[-1]\n",
    "gmsh.finalize()\n",
    "\n",
    "print(scaler.inverse_transform(internal_node_prediction), actual_value)\n"
   ]
  }
 ]
}