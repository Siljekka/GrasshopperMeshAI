{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.path as mpl_path\n",
    "import matplotlib.pyplot as plt\n",
    "import pre_processing as pp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and methods for GridPoint-manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "|---------------------------------------------------\n",
    "|       CONSTANTS\n",
    "|---------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "GRID_RESOLUTION = 20\n",
    "PATCH_SIZE = 2\n",
    "VAL_RANGE = 2.86\n",
    "GRID_SCORE_IF_OUTSIDE_CONTOUR = 1.5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "|---------------------------------------------------\n",
    "|       CLASSES AND METHODS\n",
    "|---------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class GridPoint():\n",
    "    def __init__(self, pid: int, x: float, y: float, score: float = 0):\n",
    "        self._pid = pid\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._score = score\n",
    "        self._neighbours = []\n",
    "\n",
    "    def get_coordinates(self):\n",
    "        return (self._x, self._y)\n",
    "\n",
    "    @property\n",
    "    def neighbours(self):\n",
    "        return self._neighbours\n",
    "\n",
    "    @neighbours.setter\n",
    "    def neighbours(self, neighbours: list):\n",
    "        # print(f\"set grid point ({self._x}, {self._y}) neigbours to: {neighbours}\")\n",
    "        self._neighbours = neighbours\n",
    "\n",
    "    def mean_neighbourhood_score(self):\n",
    "        # Average score of the grid point and all its neighbours\n",
    "        neighbourhood_score = self._score\n",
    "        if self._neighbours:\n",
    "            for neighbour in self._neighbours:\n",
    "                # Points on the edge have neighbours with score = 0 due to padding\n",
    "                if neighbour._score == 0:\n",
    "                    return 100\n",
    "                else:\n",
    "                    neighbourhood_score += neighbour._score\n",
    "        return neighbourhood_score/(len(self._neighbours) + 1)\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self._x\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def pid(self):\n",
    "        return self._pid\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return self._score\n",
    "\n",
    "    @score.setter\n",
    "    def score(self, score):\n",
    "        #print(f\"set grid point ({round(self._x, 3)}, {round(self._y, 3)}) to score: {round(score, 3)}\")\n",
    "        self._score = score\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"{round(self._score, 3)}\")\n",
    "        # return (f\"({round(self._x, 3)}, {round(self._y, 3)}, {self._score})\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_point_grid():\n",
    "    \"\"\"\n",
    "    |   Output: \n",
    "    |       - a point grid, G, of resolution 20x20 filled with GridPoint-objects with coordinates\n",
    "    |         in the range ~[-1.3, -1.3] -> [1.3, 1.3]. \n",
    "    |        \n",
    "    |   The grid is structured as a list of list where both the first row and column correspond         \n",
    "    |   to the GridPoint with the lowest value (and vice verse for the last row and column).\n",
    "    |\n",
    "    |   Padding is added intermittently to simplify the generation of neighbours of a GridPoint.     \n",
    "    |   These padding points are removed before returning the list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create list of coordinates from 0->VAL_RANGE, then shift it to be mirrored on 0.\n",
    "    # Grid resolution + 2 for padding\n",
    "    x_coordinates = np.linspace(0.0, VAL_RANGE, num=GRID_RESOLUTION+2, endpoint=True) - VAL_RANGE/2\n",
    "    y_coordinates = np.linspace(0.0, VAL_RANGE, num=GRID_RESOLUTION+2, endpoint=True) - VAL_RANGE/2\n",
    "    \n",
    "    point_grid = []\n",
    "    point_id = 0\n",
    "    for y in y_coordinates:\n",
    "        point_grid_row = []\n",
    "        for x in x_coordinates:\n",
    "            point_grid_row.append(GridPoint(point_id, x, y))\n",
    "            point_id+=1\n",
    "        point_grid.append(point_grid_row)\n",
    "\n",
    "    # Sets neighbours for each grid point (not including padding)\n",
    "    for y in range(1, len(point_grid)-1):\n",
    "        for x in range(1, len(point_grid)-1):\n",
    "            neighbour_list = []\n",
    "            neighbour_list.append(point_grid[y-1][x-1])     # bottom left\n",
    "            neighbour_list.append(point_grid[y-1][x])       # bottom\n",
    "            neighbour_list.append(point_grid[y-1][x+1])     # bottom right\n",
    "            neighbour_list.append(point_grid[y][x+1])       # right\n",
    "            neighbour_list.append(point_grid[y+1][x+1])     # top right\n",
    "            neighbour_list.append(point_grid[y+1][x])       # top\n",
    "            neighbour_list.append(point_grid[y+1][x-1])     # top left\n",
    "            neighbour_list.append(point_grid[y][x-1])       # left\n",
    "            point_grid[y][x].neighbours = neighbour_list\n",
    "\n",
    "\n",
    "    # Removes padding points from output by removing points that were not given neighbours\n",
    "    cleaned_point_grid = []\n",
    "    for row in point_grid:\n",
    "        cleaned_row = [x for x in row if x.neighbours]\n",
    "        # Completely empty rows (first and last) are also removed\n",
    "        if cleaned_row:\n",
    "            cleaned_point_grid.append(cleaned_row)\n",
    "    \n",
    "    return cleaned_point_grid\n",
    "\n",
    "\n",
    "def calculate_score(point_grid, internal_nodes, contour_points) -> None:\n",
    "    \"\"\"\n",
    "    |   Input: \n",
    "    |       - a point grid (list of lists) \n",
    "    |       - coordinates of the internal nodes in the mesh (list of tuples)\n",
    "    |       - coordinates of the contour vertices (list of tuples)\n",
    "    |\n",
    "    |   For each GridPoint in the point grid, we iterate over the internal nodes and calculate\n",
    "    |   the euclidean distances (scores) to each internal node. The shortest distance is the\n",
    "    |   score of the GridPoint. \n",
    "    |\n",
    "    |   If a GridPoint is not inside the contour, we set it's score to an arbitrary penalty. \n",
    "    |\n",
    "    |\n",
    "    \"\"\"\n",
    "    # Define contour (for finding points inside)\n",
    "    contour_path = mpl_path.Path(contour_points)\n",
    "\n",
    "    for row in point_grid:\n",
    "        for point in row:\n",
    "            score = 100\n",
    "            point_coordinates = point.get_coordinates()\n",
    "\n",
    "            for internal_node in internal_nodes:\n",
    "                distance = sqrt((point_coordinates[0]-internal_node[0])\n",
    "                                ** 2 + (point_coordinates[1] - internal_node[1])**2)\n",
    "                if distance < score:\n",
    "                        score = distance\n",
    "            if contour_path.contains_point(point_coordinates):\n",
    "                if score < 1:\n",
    "                    point.score = score**1.2\n",
    "                else:\n",
    "                    point.score = score\n",
    "            else:\n",
    "                point.score = 2*score\n",
    "            \n",
    "\n",
    "\n",
    "def generate_patches(point_grid):\n",
    "    \"\"\"\n",
    "    |   Generates patches of the given patch size (NxN) containing the GridPoint-objects\n",
    "    |   of the patch.\n",
    "    \"\"\"\n",
    "\n",
    "    patches = []\n",
    "    for row in range(0, GRID_RESOLUTION, PATCH_SIZE):\n",
    "        for col in range(0, GRID_RESOLUTION, PATCH_SIZE):\n",
    "            p1 = point_grid[row][col]\n",
    "            p2 = point_grid[row][col+1]\n",
    "            p3 = point_grid[row+1][col]\n",
    "            p4 = point_grid[row+1][col+1]\n",
    "            \n",
    "            patches.append([p1, p2, p3, p4])\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_training_dataset_from_contour(contour, point_grid):\n",
    "    \"\"\"\n",
    "    |   Input:\n",
    "    |       - coordinates of a contour\n",
    "    |       - a point grid\n",
    "    |\n",
    "    |   Output: \n",
    "    |       - a list of list where each row of the list contains data for one patch:\n",
    "    |           - the input contour coordinates [x1 y1 ... xn yn], \n",
    "    |           - the coordinates of the corners of the patch [x1 y1 ... x4 y4]\n",
    "    |           - the grid scores of the corners of the patch [gs1 gs2 gs3 gs4]\n",
    "    \"\"\"\n",
    "\n",
    "    patches = generate_patches(point_grid)\n",
    "\n",
    "    dataset = []\n",
    "    for patch in patches:\n",
    "        patch_data = []\n",
    "        \n",
    "        patch_coordinates = [coordinate for point in patch for coordinate in point.get_coordinates()]\n",
    "        patch_grid_scores = [point.score for point in patch]\n",
    "\n",
    "        patch_data.extend(contour)\n",
    "        patch_data.extend(patch_coordinates)\n",
    "        patch_data.extend(patch_grid_scores)\n",
    "\n",
    "        dataset.append(patch_data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_patch_collection(dataset):\n",
    "    \"\"\"\n",
    "    !!! only functional for 6gons with 2 internal nodes !!!\n",
    "    |   Input:\n",
    "    |       - a pandas.DataFrame where each row consists of:\n",
    "    |           - the coordinates of a contour\n",
    "    |           - the coordinates of a set number of internal nodes\n",
    "    |\n",
    "    |   Output: \n",
    "    |       - a list of list where the rows contains all the \"patch data\" from all the given contours\n",
    "    |\n",
    "    |   Used for csv generation; feeds into write_patch_collection_to_csv.\n",
    "    |\n",
    "    \"\"\"\n",
    "    \n",
    "    patch_collection = []   \n",
    "    for row in dataset.values.tolist():\n",
    "        # For a 6-gon with 2 internal nodes, a row is structured like:\n",
    "        # contour_coordinates [x1 -> y6] (12 values)\n",
    "        # internal_node_count = 2 (1 value)\n",
    "        # internal_node_coordinates [x1 -> y2] (4 values)\n",
    "        contour_coordinates_flat_list = row[:12]\n",
    "        internal_nodes_list = row[-4:]\n",
    "\n",
    "        # Turn flat list into list of tuples (used in calculate_score)\n",
    "        contour = list(zip(contour_coordinates_flat_list[::2], contour_coordinates_flat_list[1::2]))\n",
    "        internal_nodes = list(zip(internal_nodes_list[::2], internal_nodes_list[1::2]))\n",
    "\n",
    "        point_grid = generate_point_grid()\n",
    "        calculate_score(point_grid, internal_nodes, contour)\n",
    "\n",
    "        contour_patches = generate_patch_training_dataset_from_contour(contour_coordinates_flat_list, point_grid)\n",
    "\n",
    "        patch_collection.extend(contour_patches)\n",
    "        \n",
    "    return patch_collection\n",
    "\n",
    "# Method for writing a patch collection to csv\n",
    "def write_patch_collection_to_csv(patch_collection, filename) -> None:\n",
    "    with open(filename, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['x1', 'y1',\n",
    "                'x2', 'y2',\n",
    "                'x3', 'y3',\n",
    "                'x4', 'y4',\n",
    "                'x5', 'y5',\n",
    "                'x6', 'y6',\n",
    "                'gx1', 'gy1',\n",
    "                'gx2', 'gy2',\n",
    "                'gx3', 'gy3',\n",
    "                'gx4', 'gy4',\n",
    "                'sg1', 'sg2',\n",
    "                'sg3', 'sg4'\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for row in patch_collection:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def generate_internal_nodes_from_grid_score(point_grid, target_internal_node_count: int) -> list:\n",
    "    \"\"\"\n",
    "    |   Input:\n",
    "    |       - point grid with calculated/precited grid scores\n",
    "    |       - target number of internal nodes within a contour,\n",
    "    | \n",
    "    |   Output:\n",
    "    |       - a list of the (x, y)-coordinates of the internal nodes interpolated from the grid scores.\n",
    "    |\n",
    "    |-------------------------------\n",
    "    |   Interpolation\n",
    "    |-------------------------------\n",
    "    |   The interpolation process is based on two ideas. Given a grid point with minimal score, we assume;\n",
    "    |    1. The actual point lies within the quadrant that contains the diagonal neighbour with the lowest score.\n",
    "    |    2. The location of the actual point within the quadrant can be approximated using a procedure similar\n",
    "    |       to how one would find the center of mass in mechanics. We weight each node in the quadrant using the \n",
    "    |       inverse square of their grid scores.\n",
    "    \"\"\"\n",
    "    internal_nodes = []\n",
    "    point_grid = [point for row in point_grid for point in row]  # flattened point grid\n",
    "\n",
    "    # The quadrants-dictionary contains the indices of all the points in a \n",
    "    # quadrant given the index of a \"diagonal\" neighbour to the GridPoint-object.\n",
    "    # 6  5  4\n",
    "    # 7  P  3\n",
    "    # 0  1  2\n",
    "    quadrants = {\n",
    "        0: [7, 0, 1],\n",
    "        2: [1, 2, 3],\n",
    "        4: [3, 4, 5],\n",
    "        6: [5, 6, 7],\n",
    "    }\n",
    "\n",
    "    while len(internal_nodes) < target_internal_node_count:\n",
    "\n",
    "        min_grid_point = min(point_grid, key=lambda x: x.score)\n",
    "        neighbours = min_grid_point.neighbours\n",
    "        \n",
    "        # Try without quadrant (use all neighbours)\n",
    "        # # Decides which quadrant the point is located in. \n",
    "        # lowest_score_diagonal_node = neighbours[0]\n",
    "        # lowest_score_diagonal_node_index = 0\n",
    "        # for i in [2, 4, 6]:\n",
    "        #     if neighbours[i].score < lowest_score_diagonal_node.score:\n",
    "        #         lowest_score_diagonal_node = neighbours[i]\n",
    "        #         lowest_score_diagonal_node_index = i\n",
    "\n",
    "        \n",
    "        # # Generates a list containing the grid points in the quadrant.\n",
    "        # gridpoint_quadrant = [neighbours[i] for i in quadrants[lowest_score_diagonal_node_index]]\n",
    "        # gridpoint_quadrant.append(min_grid_point)\n",
    "\n",
    "        gridpoint_quadrant = [neighbour for neighbour in min_grid_point.neighbours]\n",
    "        gridpoint_quadrant.append(min_grid_point)\n",
    "\n",
    "        # Calculate weights of each node in the quadrant.\n",
    "        total_score = sum(point.score**16 for point in gridpoint_quadrant)\n",
    "        weights = np.array([(total_score-point.score**16)/total_score for point in gridpoint_quadrant])\n",
    "        total_weight = sum(weights)\n",
    "\n",
    "        # Interpolation\n",
    "        interpolated_x = 0\n",
    "        interpolated_y = 0\n",
    "        for i, p in enumerate(gridpoint_quadrant):\n",
    "            interpolated_x += weights[i]*p.x\n",
    "            interpolated_y += weights[i]*p.y\n",
    "        interpolated_x /= total_weight\n",
    "        interpolated_y /= total_weight\n",
    "\n",
    "        internal_nodes.append((interpolated_x, interpolated_y))\n",
    "\n",
    "        # We remove the current minimum GridPoint and all its neighbours from the point grid.\n",
    "        if len(internal_nodes) != target_internal_node_count:\n",
    "            points_to_exclude_by_id = {min_grid_point.pid}\n",
    "            points_to_exclude_by_id.update([point.pid for point in neighbours])\n",
    "            \n",
    "            point_grid[:] = [point for point in point_grid if point.pid not in points_to_exclude_by_id]\n",
    "\n",
    "    return internal_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction pipeline for case: 6-gon with two internal nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_internal_node_count() -> int:\n",
    "    return 2\n",
    "\n",
    "\n",
    "# Pre-processing method for nn2. \n",
    "def prediction_pipeline_nn2(contour, internal_node_count):\n",
    "\n",
    "    # Predict internal node count using neural network 1.\n",
    "    internal_node_count = predict_internal_node_count()\n",
    "\n",
    "    # Build point grid\n",
    "    point_grid = generate_point_grid()\n",
    "    patches = generate_patches(point_grid)\n",
    "\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour'] \n",
    "    transformed_contour_data = [coordinate for point in transformed_contour for coordinate in point]\n",
    "\n",
    "    # Load model from file\n",
    "    model = tf.keras.models.load_model('model/six_edge_2x_outside_many_nodes_big')\n",
    "    # model.summary()\n",
    "\n",
    "    # Get patch data\n",
    "    for patch in patches:\n",
    "        patch_data = [coordinate for point in patch for coordinate in point.get_coordinates()]\n",
    "\n",
    "        # Define prediction data\n",
    "        features = np.append(transformed_contour, patch_data)\n",
    "        prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        prediction_result = model(prediction_data).numpy()\n",
    "\n",
    "        # Write predicted score to the point grid\n",
    "        for i, p in enumerate(patch):\n",
    "            p.score = prediction_result[0][i] \n",
    "\n",
    "    return point_grid\n",
    "\n",
    "\n",
    "# Naive way of only checking the results on contours that the reference mesher would insert two points into\n",
    "meshed_internal_node_count = -1\n",
    "internal_node_count = predict_internal_node_count()\n",
    "\n",
    "while meshed_internal_node_count != internal_node_count:\n",
    "    contour = pp.create_random_ngon(6)\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour']\n",
    "    \n",
    "    predicted_point_grid = prediction_pipeline_nn2(transformed_contour, internal_node_count)\n",
    "    internal_nodes = generate_internal_nodes_from_grid_score(predicted_point_grid, internal_node_count)\n",
    "\n",
    "\n",
    "    meshed_contour = pp.mesh_contour(transformed_contour, 0.6)\n",
    "    meshed_internal_points = meshed_contour[-4:]\n",
    "    meshed_internal_node_count = int(meshed_contour[13])\n",
    "    print(f\"meshed contour has {meshed_contour[13]} internal nodes!\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "|-------------------------\n",
    "|       PLOTTING\n",
    "|-------------------------\n",
    "\"\"\"\n",
    "flat_pgp = [point for row in predicted_point_grid for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "plt.grid(b=True)\n",
    "pl1 = plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "pl2 = plt.plot([meshed_internal_points[0], meshed_internal_points[2]], \n",
    "         [meshed_internal_points[1], meshed_internal_points[3]], 'gD',\n",
    "            label=\"Internal nodes from reference mesher\",\n",
    "            )\n",
    "\n",
    "# Plot transformed contour\n",
    "pl3 = pp.plot_polygon(transformed_contour, style='r')\n",
    "\n",
    "# Plot point grid\n",
    "pl4 = plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "internal_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshed_internal_points ,predicted_point_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to scatter plot grid points to see whats going on\n",
    "flat_pgp = [point for row in predicted_point_grid for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "plt.grid(b=True)\n",
    "plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "plt.plot(meshed_internal_points[0], meshed_internal_points[1], 'gD')\n",
    "plt.plot(meshed_internal_points[2], meshed_internal_points[3], 'gD')\n",
    "\n",
    "# Plot transformed contour\n",
    "pp.plot_polygon(transformed_contour, style='r')\n",
    "\n",
    "# Plot point grid\n",
    "plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "point_grid_test = generate_point_grid()\n",
    "test_points = [(-0.234, -0.234), (0.234, 0.234)]\n",
    "transformed_contour = pp.procrustes(pp.create_random_ngon(6))['transformed_contour']\n",
    "calculate_score(point_grid_test, test_points, transformed_contour)\n",
    "\n",
    "internal_nodes = generate_internal_nodes_from_grid_score(point_grid_test, len(test_points))\n",
    "\n",
    "# Try to scatter plot grid points to see whats going on\n",
    "flat_pgp = [point for row in point_grid_test for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "plt.grid(b=True)\n",
    "plt.plot([x[0] for x in test_points], [y[1] for y in test_points], 'gD')\n",
    "plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "\n",
    "# Plot transformed contour\n",
    "pp.plot_polygon(transformed_contour, style='r')\n",
    "\n",
    "# Plot point grid\n",
    "plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset for patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read meshed contours to dataframe\n",
    "df = pd.read_csv('data/6-gon-mesh-with-internal-nodes.csv')\n",
    "\n",
    "# Csv-file to write to.\n",
    "new_csv_path = \"data/patches-dataset-2x-outside.csv\"\n",
    "\n",
    "# 2. Separate based on internal nodes added. We choose 2 as it has the highest incidence.\n",
    "#    -> Remove empty columns with dropna.\n",
    "#    -> Remove target_edge_length column (experiment)\n",
    "df_two_internal = df[df.internal_node_count == 2.0].dropna(axis=1, how='all')\n",
    "df_two_internal = df_two_internal.drop(\"target_edge_length\", axis=1)\n",
    "\n",
    "# 3. Different 'target_edge_length's can produce same mesh.\n",
    "#    -> Since we remove differentiation on target_edge_length, we should also\n",
    "#    -> remove these duplicates. \n",
    "df_two_internal_no_dupes = df_two_internal.drop_duplicates()\n",
    "\n",
    "dataset = df_two_internal_no_dupes\n",
    "write_patch_collection_to_csv(generate_patch_collection(dataset), new_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network 2: predicting internal node positions (or rather: predict grid score)\n",
    "\n",
    "For now we only train the network on the dataset with:\n",
    "- 6 edges\n",
    "- 2 internal nodes\n",
    "- 100 2x2 patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "from tensorflow.keras import layers, metrics\n",
    "import sklearn.utils\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "#  %load_ext tensorboard\n",
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "model_path = 'model/six_edge_2x_outside_many_nodes_big'\n",
    "patches = pd.read_csv('data/patches-dataset-2x-outside-big.csv')\n",
    "# patches = pd.read_csv('data/patches-dataset-test.csv')\n",
    "\n",
    "# Split dataset into 70/15/15 training/validation/test\n",
    "patches_train = patches.sample(frac=0.85, random_state=0)\n",
    "patches_test = patches.drop(patches_train.index)\n",
    "\n",
    "# Shuffle dataset\n",
    "patches_train = sklearn.utils.shuffle(patches_train)\n",
    "patches_test = sklearn.utils.shuffle(patches_test)\n",
    "\n",
    "# Split dataset into features and labels; last 4 columns \n",
    "# (predicted grid scores of a patch) are the labels.\n",
    "train_features = patches_train.iloc[:, :-4]\n",
    "train_labels = patches_train.iloc[:, -4:]\n",
    "\n",
    "test_features = patches_train.iloc[:, :-4]\n",
    "test_labels = patches_train.iloc[:, -4:]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(20,)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(96),\n",
    "    layers.Activation('relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(64),\n",
    "    layers.Activation('relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(4),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "decay_steps= 10_000\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(LEARNING_RATE, decay_steps, 1e-4)\n",
    "\n",
    "\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              optimizer=tf.optimizers.Adam(\n",
    "                  learning_rate=lr_schedule,\n",
    "              ),\n",
    "              metrics=[\n",
    "                  metrics.mean_absolute_error,\n",
    "                  metrics.mean_absolute_percentage_error\n",
    "              ],\n",
    "              )\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.18,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks_list,\n",
    "                    )\n",
    "\n",
    "# Evaluate the model\n",
    "train_acc, train_abs_err, train_abs_pct_err = model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc, test_abs_err, test_abs_pct_err = model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))\n",
    "print(f\"Training mean absolute and percentage error: {(round(train_abs_err, 3), train_abs_pct_err)}\")\n",
    "print(f\"Test mean absolute and percentage error: {(test_abs_err, test_abs_pct_err)}\")\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model/model_path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf"
  },
  "kernelspec": {
   "name": "python374jvsc74a57bd0ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf",
   "display_name": "Python 3.7.4 64-bit ('masterMLvenv': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "5206f4284a6f5b0c767212dafd38c1f4877780ac1730cf1f3a311f04c6a7f3fd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}