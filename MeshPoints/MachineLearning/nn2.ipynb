{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf",
   "display_name": "Python 3.7.4 64-bit ('masterMLvenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5206f4284a6f5b0c767212dafd38c1f4877780ac1730cf1f3a311f04c6a7f3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.path as mpl_path\n",
    "import matplotlib.pyplot as plt\n",
    "import pre_processing as pp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "source": [
    "## Classes and methods for GridPoint-manipulation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridPoint():\n",
    "    def __init__(self, pid: int, x: float, y: float, score: float = 0):\n",
    "        self._pid = pid\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._score = score\n",
    "        self._neighbours = []\n",
    "\n",
    "    def get_coordinates(self):\n",
    "        return (self._x, self._y)\n",
    "\n",
    "    @property\n",
    "    def neighbours(self):\n",
    "        return self._neighbours\n",
    "\n",
    "    @neighbours.setter\n",
    "    def neighbours(self, neighbours: list):\n",
    "        # print(f\"set grid point ({self._x}, {self._y}) neigbours to: {neighbours}\")\n",
    "        self._neighbours = neighbours\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self._x\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def pid(self):\n",
    "        return self._pid\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return self._score\n",
    "\n",
    "    @score.setter\n",
    "    def score(self, score):\n",
    "        #print(f\"set grid point ({round(self._x, 3)}, {round(self._y, 3)}) to score: {round(score, 3)}\")\n",
    "        self._score = score\n",
    "\n",
    "    def __repr__(self):\n",
    "        # return (f\"{self._pid}\")\n",
    "        return (f\"({round(self._x, 3)}, {round(self._y, 3)}, {self._score})\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_point_grid():\n",
    "    \"\"\"\n",
    "    |   Output: \n",
    "    |       - a point grid, G, of resolution 20x20 filled with GridPoint-objects with coordinates\n",
    "    |         in the range ~[-1.3, -1.3] -> [1.3, 1.3]. \n",
    "    |        \n",
    "    |   The grid is structured as a list of list where both the first row and column correspond         \n",
    "    |   to the GridPoint with the lowest value (and vice verse for the last row and column).\n",
    "    |\n",
    "    |   Padding is added intermittently to simplify the generation of neighbours of a GridPoint.     \n",
    "    |   These padding points are removed before returning the list.\n",
    "    \n",
    "    !!! function is hardcoded to 20x20 in the range ~[-1.3, -1.3] -> [1.3, 1.3] to stop myself from messing up !!!\n",
    "    \"\"\"\n",
    "\n",
    "    VAL_RANGE = 2.86\n",
    "    GRID_RESOLUTION = 20\n",
    "\n",
    "    # Create list of coordinates from 0->VAL_RANGE, then shift it to be mirrored on 0.\n",
    "    # Grid resolution + 2 for padding\n",
    "    x_coordinates = np.linspace(0.0, VAL_RANGE, num=GRID_RESOLUTION+2, endpoint=True) - VAL_RANGE/2\n",
    "    y_coordinates = np.linspace(0.0, VAL_RANGE, num=GRID_RESOLUTION+2, endpoint=True) - VAL_RANGE/2\n",
    "    \n",
    "    point_grid = []\n",
    "    point_id = 0\n",
    "    for y in y_coordinates:\n",
    "        point_grid_row = []\n",
    "        for x in x_coordinates:\n",
    "            point_grid_row.append(GridPoint(point_id, x, y))\n",
    "            point_id+=1\n",
    "        point_grid.append(point_grid_row)\n",
    "\n",
    "    # Sets neighbours for each grid point (not including padding)\n",
    "    for y in range(1, len(point_grid)-1):\n",
    "        for x in range(1, len(point_grid)-1):\n",
    "            neighbour_list = []\n",
    "            neighbour_list.append(point_grid[y-1][x-1])     # bottom left\n",
    "            neighbour_list.append(point_grid[y-1][x])       # bottom\n",
    "            neighbour_list.append(point_grid[y-1][x+1])     # bottom right\n",
    "            neighbour_list.append(point_grid[y][x+1])       # right\n",
    "            neighbour_list.append(point_grid[y+1][x+1])     # top right\n",
    "            neighbour_list.append(point_grid[y+1][x])       # top\n",
    "            neighbour_list.append(point_grid[y+1][x-1])     # top left\n",
    "            neighbour_list.append(point_grid[y][x-1])       # left\n",
    "            point_grid[y][x].neighbours = neighbour_list\n",
    "\n",
    "\n",
    "    # Removes padding points from output by removing points that were not given neighbours\n",
    "    cleaned_point_grid = []\n",
    "    for row in point_grid:\n",
    "        cleaned_row = [x for x in row if x.neighbours]\n",
    "        # Completely empty rows (first and last) are also removed\n",
    "        if cleaned_row is not None:\n",
    "            cleaned_point_grid.append(cleaned_row)\n",
    "    \n",
    "    return cleaned_point_grid\n",
    "\n",
    "\n",
    "def calculate_score(point_grid, internal_nodes, contour_points) -> None:\n",
    "    \"\"\"\n",
    "    |   Input: \n",
    "    |       - a point grid (list of lists) \n",
    "    |       - coordinates of the internal nodes in the mesh (list of tuples)\n",
    "    |       - coordinates of the contour vertices (list of tuples)\n",
    "    |\n",
    "    |   For each GridPoint in the point grid, we iterate over the internal nodes and calculate\n",
    "    |   the euclidean distances (scores) to each internal node. The shortest distance is the\n",
    "    |   score of the GridPoint. \n",
    "    |\n",
    "    |   If a GridPoint is not inside the contour, we set it's score = 2. \n",
    "    |\n",
    "    \"\"\"\n",
    "    # Define contour (for finding points inside)\n",
    "    contour_path = mpl_path.Path(contour_points)\n",
    "\n",
    "    for row in point_grid:\n",
    "        for point in row:\n",
    "            score = 2\n",
    "            point_coordinates = point.get_coordinates()\n",
    "            if contour_path.contains_point(point_coordinates):\n",
    "                for internal_node in internal_nodes:\n",
    "                    distance = sqrt((point_coordinates[0]-internal_node[0])\n",
    "                                    ** 2 + (point_coordinates[1] - internal_node[1])**2)\n",
    "                    if distance < score:\n",
    "                        score = distance\n",
    "            point.score = score\n",
    "\n",
    "\n",
    "def generate_patch_training_dataset_from_contour(contour, point_grid):\n",
    "    \"\"\"\n",
    "    |   Input:\n",
    "    |       - coordinates of a contour\n",
    "    |       - a point grid\n",
    "    |\n",
    "    |   Output: \n",
    "    |       - a list of list where each row of the list contains data for one patch:\n",
    "    |           - the input contour coordinates [x1 y1 ... xn yn], \n",
    "    |           - the coordinates of the corners of the patch [x1 y1 ... x4 y4]\n",
    "    |           - the grid scores of the corners of the patch [gs1 gs2 gs3 gs4]\n",
    "    \"\"\"\n",
    "\n",
    "    patch_size = 2\n",
    "    grid_resolution = 20\n",
    "\n",
    "    patches = []\n",
    "    for row in range(0, grid_resolution, patch_size):\n",
    "        for col in range(0, grid_resolution, patch_size):\n",
    "            p1 = point_grid[row][col]\n",
    "            p2 = point_grid[row][col+1]\n",
    "            p3 = point_grid[row+1][col]\n",
    "            p4 = point_grid[row+1][col+1]\n",
    "            \n",
    "            patch_coordinates = [p1.x, p1.y, p2.x, p2.y,\n",
    "                                 p3.x, p3.y, p4.x, p4.y]\n",
    "            \n",
    "            patch = contour\n",
    "            patch.extend(patch_coordinates)\n",
    "            patch.extend([p1.score, p2.score, p3.score, p4.score])\n",
    "            \n",
    "            patches.append(patch)\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_collection(dataset):\n",
    "    \"\"\"\n",
    "    !!! only functional for 6gons with 2 internal nodes !!!\n",
    "    |   Input:\n",
    "    |       - a pandas.DataFrame where each row consists of:\n",
    "    |           - the coordinates of a contour\n",
    "    |           - the coordinates of a set number of internal nodes\n",
    "    |\n",
    "    |   Output: \n",
    "    |       - a list of list where the rows contains all the \"patch data\" from all the given contours\n",
    "    |\n",
    "    |   Used for csv generation; feeds into write_patch_collection_to_csv.\n",
    "    |\n",
    "    \"\"\"\n",
    "    \n",
    "    patch_collection = []   \n",
    "    for row in dataset.values.tolist():\n",
    "        # For a 6-gon with 2 internal nodes, a row is structured like:\n",
    "        # contour_coordinates [x1 -> y6] (12 values)\n",
    "        # internal_node_count = 2 (1 value)\n",
    "        # internal_node_coordinates [x1 -> y2] (4 values)\n",
    "        contour_coordinates_flat_list = row[:12]\n",
    "        internal_nodes_list = row[-4:]\n",
    "\n",
    "        # Turn flat list into list of tuples (used in calculate_score)\n",
    "        contour = list(zip(contour_coordinates_flat_list[::2], contour_coordinates_flat_list[1::2]))\n",
    "        internal_nodes = list(zip(internal_nodes_list[::2], internal_nodes_list[1::2]))\n",
    "\n",
    "        point_grid = generate_point_grid()\n",
    "        calculate_score(point_grid, internal_nodes, contour)\n",
    "\n",
    "        contour_patches = generate_patch_training_dataset_from_contour(contour_coordinates_flat_list, point_grid)\n",
    "\n",
    "        patch_collection.extend(contour_patches)\n",
    "        \n",
    "    return patch_collection\n",
    "\n",
    "# Method for writing a patch collection to csv\n",
    "def write_patch_collection_to_csv(patch_collection):\n",
    "    with open(f\"data/patches-dataset.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        header = ['x1', 'y1',\n",
    "        'x2', 'y2',\n",
    "        'x3', 'y3',\n",
    "        'x4', 'y4',\n",
    "        'x5', 'y5',\n",
    "        'x6', 'y6',\n",
    "        'gx1', 'gy1',\n",
    "        'gx2', 'gy2',\n",
    "        'gx3', 'gy3',\n",
    "        'gx4', 'gy4',\n",
    "        'sg1', 'sg2',\n",
    "        'sg3', 'sg4'\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for row in patch_collection:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def generate_internal_nodes_from_grid_score(point_grid, target_internal_node_count: int) -> list:\n",
    "    \"\"\"\n",
    "    |   Input:\n",
    "    |       - point grid with calculated/precited grid scores\n",
    "    |       - target number of internal nodes within a contour,\n",
    "    | \n",
    "    |   Output:\n",
    "    |       - a list of the (x, y)-coordinates of the internal nodes interpolated from the grid scores.\n",
    "    |\n",
    "    |-------------------------------\n",
    "    |   Interpolation\n",
    "    |-------------------------------\n",
    "    |   The interpolation process is based on two ideas. Given a grid point with minimal score, we assume;\n",
    "    |    1. The actual point lies within the quadrant that contains the diagonal neighbour with the lowest score.\n",
    "    |    2. The location of the actual point within the quadrant can be approximated using a procedure similar\n",
    "    |       to how one would find the center of mass in mechanics. We weight each node in the quadrant using the \n",
    "    |       inverse square of their grid scores.\n",
    "    \"\"\"\n",
    "    internal_nodes = []\n",
    "    point_grid = [point for row in point_grid for point in row]  # flattened point grid\n",
    "\n",
    "    # The quadrants-dictionary contains the indices of all the points in a \n",
    "    # quadrant given the index of a \"diagonal\" neighbour to the GridPoint-object.\n",
    "    # 6  5  4\n",
    "    # 7  P  3\n",
    "    # 0  1  2\n",
    "    quadrants = {\n",
    "        0: [7, 0, 1],\n",
    "        2: [1, 2, 3],\n",
    "        4: [3, 4, 5],\n",
    "        6: [5, 6, 7],\n",
    "    }\n",
    "\n",
    "    while len(internal_nodes) < target_internal_node_count:\n",
    "\n",
    "        min_grid_point = min(point_grid, key=lambda x: x.score)\n",
    "        neighbours = min_grid_point.neighbours\n",
    "        \n",
    "        # Decides which quadrant the point is located in. \n",
    "        lowest_score_diagonal_node = neighbours[0]\n",
    "        lowest_score_diagonal_node_index = 0\n",
    "        for i in [2, 4, 6]:\n",
    "            if neighbours[i].score < lowest_score_diagonal_node.score:\n",
    "                lowest_score_diagonal_node = neighbours[i]\n",
    "                lowest_score_diagonal_node_index = i\n",
    "\n",
    "        \n",
    "        # Generates a list containing the grid points in the quadrant.\n",
    "        gridpoint_quadrant = [neighbours[i] for i in quadrants[lowest_score_diagonal_node_index]]\n",
    "        gridpoint_quadrant.append(min_grid_point)\n",
    "\n",
    "        # Calculate weights of each node in the quadrant.\n",
    "        total_score = sum(point.score**16 for point in gridpoint_quadrant)\n",
    "        weights = np.array([(total_score-point.score**16)/total_score for point in gridpoint_quadrant])\n",
    "        total_weight = sum(weights)\n",
    "\n",
    "        # Interpolation\n",
    "        interpolated_x = 0\n",
    "        interpolated_y = 0\n",
    "        for i, p in enumerate(gridpoint_quadrant):\n",
    "            interpolated_x += weights[i]*p.x\n",
    "            interpolated_y += weights[i]*p.y\n",
    "        interpolated_x /= total_weight\n",
    "        interpolated_y /= total_weight\n",
    "\n",
    "        internal_nodes.append((interpolated_x, interpolated_y))\n",
    "\n",
    "        # We remove the current minimum GridPoint and all its neighbours from the point grid.\n",
    "        if len(internal_nodes) != target_internal_node_count:\n",
    "            points_to_exclude_by_id = {min_grid_point.pid}\n",
    "            points_to_exclude_by_id.update([point.pid for point in neighbours])\n",
    "            \n",
    "            point_grid[:] = [point for point in point_grid if point.pid not in points_to_exclude_by_id]\n",
    "\n",
    "    return internal_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "point_grid = generate_point_grid()\n",
    "test_points = [(-0.123, -0.234), (0.123, 0.234), (0.4, -0.2), (-0.432, 0.3)]\n",
    "contour = pp.procrustes(pp.create_random_ngon(6))['transformed_contour']\n",
    "calculate_score(point_grid, test_points, contour)\n",
    "\n",
    "internal_nodes = generate_internal_nodes_from_grid_score(point_grid, 4)\n",
    "\n",
    "# Plot target point (green) and min grid point (red)\n",
    "plt.plot([x[0] for x in test_points], [y[1] for y in test_points], 'gD')\n",
    "plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "pp.plot_polygon(contour)\n",
    "plt.grid(b=True)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Prediction pipeline for case: 6-gon with two internal nodes\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a random contour.\n",
    "# 2. Set the amount of internal nodes to be predicted (2)\n",
    "# 3. Create a point grid and divide it into patches.\n",
    "# 4. Feed patches along with contour coordinates into model, populate point grid with predicted grid scores.\n",
    "# 5. Place and interpolate points based on the predicted grid scores.\n",
    "# 6. Evaluate results, see if it isnt completely stupid:\n",
    "#    - Try meshing the same contour with gmsh to see where the internal nodes are \"supposed\" to be. "
   ]
  },
  {
   "source": [
    "# Creating dataset for patches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read meshed contours to dataframe\n",
    "df = pd.read_csv('data/6-gon-mesh-with-internal-nodes.csv')\n",
    "\n",
    "# 2. Separate based on internal nodes added. We choose 2 as it has the highest incidence.\n",
    "#    -> Remove empty columns with dropna.\n",
    "#    -> Remove target_edge_length column (experiment)\n",
    "df_two_internal = df[df.internal_node_count == 2.0].dropna(axis=1, how='all')\n",
    "df_two_internal = df_two_internal.drop(\"target_edge_length\", axis=1)\n",
    "\n",
    "# 3. Different 'target_edge_length's can produce same mesh.\n",
    "#    -> Since we remove differentiation on target_edge_length, we should also\n",
    "#    -> remove these duplicates. \n",
    "df_two_internal_no_dupes = df_two_internal.drop_duplicates()\n",
    "\n",
    "dataset = df_two_internal_no_dupes\n",
    "# patches_to_csv(generate_patch_collection(dataset))\n"
   ]
  },
  {
   "source": [
    "# Neural network 2: predicting internal node positions (or rather: predict grid score)\n",
    "\n",
    "For now we only train the network on the dataset with:\n",
    "- 6 edges\n",
    "- 2 internal nodes\n",
    "- 100 2x2 patches\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "from tensorflow.keras import layers, metrics\n",
    "import sklearn.utils\n",
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "patches = pd.read_csv('data/patches-dataset.csv')\n",
    "patches = sklearn.utils.shuffle(patches)\n",
    "\n",
    "# Split dataset into 70/15/15 training/validation/test\n",
    "patches_train = patches.sample(frac=0.85, random_state=0)\n",
    "patches_test = patches.drop(patches_train.index)\n",
    "\n",
    "# Split dataset into features and labels; last 4 columns \n",
    "# (predicted grid scores of a patch) are the labels.\n",
    "train_features = patches_train.iloc[:, :-4]\n",
    "train_labels = patches_train.iloc[:, -4:]\n",
    "\n",
    "test_features = patches_train.iloc[:, :-4]\n",
    "test_labels = patches_train.iloc[:, -4:]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(20,)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(16),\n",
    "    layers.Activation('relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(16),\n",
    "    layers.Activation('relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              optimizer=tf.optimizers.Adam(),\n",
    "              )\n",
    "\n",
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.18,\n",
    "                    verbose=2,\n",
    "                    )\n",
    "\n",
    "# Evaluate the model\n",
    "train_acc = model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc = model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model/six_edge_two_internal_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}