{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.path as mpl_path\n",
    "import matplotlib.pyplot as plt\n",
    "import pre_processing as pp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from point_grid import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity testing polygon dataset. The means of all the columns should form a regular polygon. it does :)\n",
    "df = pd.read_csv('data/6-gon-correct-procrustes.csv')\n",
    "\n",
    "df = df[df.internal_nodes == 2.0].dropna(axis=1, how='all')\n",
    "df = df[df.target_edge_length == 0.4]\n",
    "df = df.drop(['target_edge_length', 'internal_nodes'], axis=1)\n",
    "means = df.mean()\n",
    "x = [means[0], means[2], means[4], means[6], means[8], means[10]]\n",
    "y = [means[1], means[3], means[5], means[7], means[9], means[11]]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set_aspect('equal')\n",
    "pp.plot_polygon(pp.create_regular_ngon(6))\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction pipeline for case: 6-gon with two internal nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_internal_node_count() -> int:\n",
    "    return 2\n",
    "\n",
    "\n",
    "# Pre-processing method for nn2. \n",
    "def prediction_pipeline_nn2(contour, internal_node_count, grid_model: tf.keras.models.Model):\n",
    "\n",
    "    # Predict internal node count using neural network 1.\n",
    "    internal_node_count = predict_internal_node_count()\n",
    "\n",
    "    # Build point grid\n",
    "    point_grid = generate_point_grid()\n",
    "    patches = generate_patches(point_grid)\n",
    "\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour'] \n",
    "    transformed_contour_data = [coordinate for point in transformed_contour for coordinate in point]\n",
    "\n",
    "    # Get patch data\n",
    "    for patch in patches:\n",
    "        patch_data = [coordinate for point in patch for coordinate in point.get_coordinates()]\n",
    "\n",
    "        # Define prediction data\n",
    "        features = np.append(transformed_contour_data, patch_data)\n",
    "        prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        prediction_result = grid_model(prediction_data).numpy()\n",
    "\n",
    "        # Write predicted score to the point grid\n",
    "        for i, p in enumerate(patch):\n",
    "            p.score = prediction_result[0][i] \n",
    "\n",
    "    return point_grid\n",
    "\n",
    "\n",
    "def direct_prediction_pipeline(contour_input_2, direct_model: tf.keras.models.Model):\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour_data = [coordinate for point in contour_input_2 for coordinate in point]\n",
    "\n",
    "    # Define prediction data\n",
    "    features = transformed_contour_data\n",
    "    direct_prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    direct_prediction_result = direct_model(direct_prediction_data).numpy()\n",
    "\n",
    "    return direct_prediction_result\n",
    "\n",
    "# =======================\n",
    "#     Initialization\n",
    "# =======================\n",
    "import gmsh\n",
    "\n",
    "grid_model_path = 'model/thesis-grid-8gon-3int-gh'\n",
    "direct_model_path = 'model/direct-internal-nodes-correct'\n",
    "gmsh.initialize()\n",
    "direct_model = tf.keras.models.load_model(direct_model_path)\n",
    "grid_model = tf.keras.models.load_model(grid_model_path)\n",
    "\n",
    "# Naive way of only checking the results on contours that the reference mesher would insert two points into\n",
    "meshed_internal_node_count = -1\n",
    "internal_node_count = predict_internal_node_count()\n",
    "\n",
    "while meshed_internal_node_count != 3:\n",
    "    contour = pp.create_random_ngon(8)\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour']\n",
    "\n",
    "    meshed_contour = pp.mesh_contour(transformed_contour, 0.4)\n",
    "    meshed_internal_points = meshed_contour[-4:]\n",
    "    meshed_internal_node_count = int(meshed_contour[13])\n",
    "    print(f\"meshed contour has {meshed_contour[13]} internal nodes!\")\n",
    "gmsh.finalize()\n",
    "\n",
    "# Grid point prediction\n",
    "predicted_point_grid = prediction_pipeline_nn2(transformed_contour, internal_node_count, grid_model)\n",
    "pg_internal_nodes = generate_internal_nodes_from_grid_score(predicted_point_grid, internal_node_count)\n",
    "\n",
    "# Direct prediction without point grid\n",
    "predicted_internal_nodes = direct_prediction_pipeline(contour, direct_model)[0]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "|-------------------------\n",
    "|       PLOTTING\n",
    "|-------------------------\n",
    "\"\"\"\n",
    "flat_pgp = [point for row in predicted_point_grid for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "# plt.grid(b=True)\n",
    "pl0 = plt.plot([meshed_internal_points[0], meshed_internal_points[2]], \n",
    "         [meshed_internal_points[1], meshed_internal_points[3]], 'gD',\n",
    "            label=\"Reference\",\n",
    "            )\n",
    "pl2 = plt.plot([predicted_internal_nodes[0], predicted_internal_nodes[2]], \n",
    "               [predicted_internal_nodes[1], predicted_internal_nodes[3]], \n",
    "               'bo',\n",
    "               label=\"Direct prediction\"\n",
    ")\n",
    "pl1 = plt.plot([x[0] for x in pg_internal_nodes], [y[1] for y in pg_internal_nodes], 'rx', label=\"Patch prediction\")\n",
    "# Plot transformed contour\n",
    "pl3 = pp.plot_polygon(transformed_contour, style='k')\n",
    "\n",
    "# Plot point grid\n",
    "pl4 = plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "plt.legend(loc='upper right', shadow='true')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.gca().set_aspect('equal')\n",
    "transformed_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted points (green)\n",
    "# plt.grid(b=True)\n",
    "pl0 = plt.plot([meshed_internal_points[0], meshed_internal_points[2]], \n",
    "         [meshed_internal_points[1], meshed_internal_points[3]], 'gD',\n",
    "            label=\"Reference\",\n",
    "            )\n",
    "pl2 = plt.plot([predicted_internal_nodes[0], predicted_internal_nodes[2]], \n",
    "               [predicted_internal_nodes[1], predicted_internal_nodes[3]], \n",
    "               'bo',\n",
    "               label=\"Direct prediction\"\n",
    ")\n",
    "pl1 = plt.plot([x[0] for x in pg_internal_nodes], [y[1] for y in pg_internal_nodes], 'rx', label=\"Patch prediction\")\n",
    "# Plot transformed contour\n",
    "pl3 = pp.plot_polygon(transformed_contour, style='k')\n",
    "\n",
    "# Plot point grid\n",
    "pl4 = plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "plt.legend(loc='upper right', shadow='true')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.gca().set_aspect('equal')\n",
    "transformed_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to scatter plot grid points to see whats going on\n",
    "flat_pgp = [point for row in predicted_point_grid for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "plt.grid(b=True)\n",
    "plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "plt.plot(meshed_internal_points[0], meshed_internal_points[1], 'gD')\n",
    "plt.plot(meshed_internal_points[2], meshed_internal_points[3], 'gD')\n",
    "\n",
    "# Plot transformed contour\n",
    "pp.plot_polygon(transformed_contour, style='r')\n",
    "\n",
    "# Plot point grid\n",
    "plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "point_grid_test = generate_point_grid()\n",
    "test_points = [(-0.234, -0.234), (0.234, 0.234)]\n",
    "transformed_contour = pp.procrustes(pp.create_random_ngon(6))['transformed_contour']\n",
    "calculate_score(point_grid_test, test_points, transformed_contour)\n",
    "\n",
    "internal_nodes = generate_internal_nodes_from_grid_score(point_grid_test, len(test_points))\n",
    "\n",
    "# Try to scatter plot grid points to see whats going on\n",
    "flat_pgp = [point for row in point_grid_test for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "# plt.grid(b=True)\n",
    "plt.plot([x[0] for x in test_points], [y[1] for y in test_points], 'rD')\n",
    "# plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "\n",
    "# Plot transformed contour\n",
    "pp.plot_polygon(transformed_contour, style='k')\n",
    "\n",
    "# Plot point grid\n",
    "plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.ocean_r, marker='s')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted points (green)\n",
    "# plt.grid(b=True)\n",
    "plt.plot([x[0] for x in test_points], [y[1] for y in test_points], 'rX')\n",
    "# plt.plot([x[0] for x in internal_nodes], [y[1] for y in internal_nodes], 'rx')\n",
    "\n",
    "# Plot transformed contour\n",
    "pp.plot_polygon(transformed_contour, style='r')\n",
    "\n",
    "# Plot point grid\n",
    "plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.YlGnBu, marker='s')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset for patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read meshed contours to dataframe\n",
    "df = pd.read_csv('data/6-gon-correct-procrustes.csv')\n",
    "\n",
    "# Csv-file to write to.\n",
    "new_csv_path = \"data/patch-data-correct-procrustes.csv\"\n",
    "\n",
    "# 2. Separate based on internal nodes added. We choose 2 as it has the highest incidence.\n",
    "#    -> Remove empty columns with dropna.\n",
    "#    -> Remove target_edge_length column (experiment)\n",
    "df = df[df.internal_nodes == 2.0].dropna(axis=1, how='all')\n",
    "dataset = df[df.target_edge_length == 0.4]\n",
    "dataset = dataset.drop(['target_edge_length', 'internal_nodes'], axis=1)\n",
    "\n",
    "write_patch_collection_to_csv(generate_patch_collection(dataset), new_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network 2: predicting internal node positions (or rather: predict grid score)\n",
    "\n",
    "For now we only train the network on the dataset with:\n",
    "- 6 edges\n",
    "- 2 internal nodes\n",
    "- 100 2x2 patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "#        DATASET \n",
    "# ======================\n",
    "import pandas as pd\n",
    "\n",
    "hexagon_path = \"data/6-gon-correct-procrustes.csv\"\n",
    "patch_path = 'data/patch-data-correct-procrustes.csv'\n",
    "dataset = pd.read_csv(patch_path)\n",
    "\n",
    "# Split dataset into 70/15/15 training/validation/test\n",
    "patch_train = dataset.sample(frac=0.85)\n",
    "patch_test = dataset.drop(patches_train.index)\n",
    "\n",
    "# Split dataset into features and labels; last 4 columns \n",
    "train_features = patch_train.iloc[:, :-4]\n",
    "train_labels = patch_train.iloc[:, -4:]\n",
    "\n",
    "test_features = patch_test.iloc[:, :-4]\n",
    "test_labels = patch_test.iloc[:, -4:]\n",
    "\n",
    "dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# ======================\n",
    "#       MODEL STUFF\n",
    "# ======================\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "model_path = 'model/thesis-grid-1'\n",
    "\n",
    "def grid_model_setup():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(20,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(4),\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = grid_model_setup()\n",
    "model.summary()\n",
    "\n",
    "decay_steps= 10_000\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(1e-1, decay_steps, 1e-3)\n",
    "\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              optimizer=tf.optimizers.Adam(\n",
    "                  learning_rate=lr_schedule,\n",
    "              ),\n",
    "              )\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=EPOCHS//5, min_delta=0.0001)\n",
    "\n",
    "\n",
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.18,\n",
    "                    verbose=2,\n",
    "                    callbacks=[checkpoint, early_stopping],\n",
    "                    )\n",
    "\n",
    "# ======================\n",
    "#       EVALUATION\n",
    "# ======================\n",
    "train_acc = model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc = model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "#  EVALUATE SAVED MODEL \n",
    "# ======================\n",
    "\n",
    "saved_model = tf.keras.models.load_model('model/thesis-grid-1')\n",
    "\n",
    "train_acc = saved_model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc = saved_model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim(0.13, 0.25)"
   ]
  },
  {
   "source": [
    "# Stat gathering for nn2\n",
    "\n",
    "1. Create random contour.\n",
    "2. Find internal mesh points with gmsh, grid nn and direct nn.\n",
    "3. Calculate how often grid/direct is outside, and the mean euclidean error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model_path = 'model/thesis-grid-1'\n",
    "direct_model_path = 'model/direct-internal-nodes-correct'\n",
    "\n",
    "\n",
    "# Pre-processing method for nn2. \n",
    "def stat_prediction_pipeline_nn2(contour, internal_node_count, grid_model):\n",
    "\n",
    "    # Build point grid\n",
    "    point_grid = generate_point_grid()\n",
    "    patches = generate_patches(point_grid)\n",
    "\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour'] \n",
    "    transformed_contour_data = [coordinate for point in transformed_contour for coordinate in point]\n",
    "\n",
    "    # Get patch data\n",
    "    for patch in patches:\n",
    "        patch_data = [coordinate for point in patch for coordinate in point.get_coordinates()]\n",
    "\n",
    "        # Define prediction data\n",
    "        features = np.append(transformed_contour_data, patch_data)\n",
    "        prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        prediction_result = grid_model(prediction_data).numpy()\n",
    "\n",
    "        # Write predicted score to the point grid\n",
    "        for i, p in enumerate(patch):\n",
    "            p.score = prediction_result[0][i] \n",
    "\n",
    "    return point_grid\n",
    "\n",
    "\n",
    "def stat_direct_prediction_pipeline(contour_input_2, direct_model):\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour_data = [coordinate for point in contour_input_2 for coordinate in point]\n",
    "\n",
    "    # Load model\n",
    "    \n",
    "\n",
    "    # Define prediction data\n",
    "    features = transformed_contour_data\n",
    "    direct_prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    direct_prediction_result = direct_model(direct_prediction_data).numpy()\n",
    "\n",
    "    return direct_prediction_result\n",
    "\n",
    "\n",
    "def calc_squared_error(contour, predicted: list, reference: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates the squared error of predictions. Assigns a negative value if point is outside of the contour.\n",
    "    Calculates the squared distance to the _closest_point.\n",
    "    \"\"\"\n",
    "    se_list = []\n",
    "    # Define contour (for finding points inside)\n",
    "    contour_path = mpl_path.Path(contour)\n",
    "\n",
    "    for p_point in predicted:\n",
    "        error = 100\n",
    "        for r_point in reference:\n",
    "            tmp_squared_error = ((p_point[0] - r_point[0]) + (p_point[1] - r_point[1]))**2\n",
    "            if tmp_squared_error < error:\n",
    "                error = tmp_squared_error\n",
    "        if contour_path.contains_point(p_point):\n",
    "            se_list.append(error)\n",
    "        else:\n",
    "            se_list.append(-error)\n",
    "    return se_list\n",
    "\n",
    "\n",
    "sample_size = 1000\n",
    "squared_error_direct = []\n",
    "squared_error_grid = []\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import gmsh\n",
    "import pre_processing as pp\n",
    "from point_grid import *\n",
    "\n",
    "# Initialization\n",
    "direct_model = tf.keras.models.load_model(direct_model_path)\n",
    "grid_model = tf.keras.models.load_model(grid_model_path)\n",
    "gmsh.initialize()\n",
    "\n",
    "for i in range(sample_size):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Gathering stats on grid model: {grid_model_path} and direct model: {direct_model_path}\")\n",
    "    print(\"\\r\", f\"processing contour {i+1} of {sample_size}\", end=\"\")\n",
    "\n",
    "    # We only check contours that the reference mesher actually place two points inside of.\n",
    "    # Target edge length: 0.4\n",
    "    meshed_internal_node_count = -1\n",
    "    target_internal_node_count = 2\n",
    "    while meshed_internal_node_count != target_internal_node_count:\n",
    "        contour = pp.create_random_ngon(6)\n",
    "        transformed_contour = pp.procrustes(contour)['transformed_contour']\n",
    "\n",
    "        meshed_contour = pp.mesh_contour(transformed_contour, 0.4)\n",
    "        meshed_internal_points = meshed_contour[-4:]\n",
    "        meshed_internal_node_count = int(meshed_contour[13])\n",
    "    \n",
    "\n",
    "    # Predict the point grid and intrapolate nodes from it.\n",
    "    predicted_point_grid = stat_prediction_pipeline_nn2(transformed_contour, target_internal_node_count, grid_model)\n",
    "    grid_prediction = generate_internal_nodes_from_grid_score(predicted_point_grid, target_internal_node_count)\n",
    "\n",
    "    # Direct prediction without point grid\n",
    "    direct_prediction_raw = stat_direct_prediction_pipeline(contour, direct_model)[0]\n",
    "    direct_prediction = list(zip(direct_prediction_raw[::2], direct_prediction_raw[1::2]))\n",
    "    \n",
    "    reference_nodes = list(zip(meshed_internal_points[::2], meshed_internal_points[1::2]))\n",
    "    # print(grid_prediction, direct_prediction, reference_nodes)\n",
    "    squared_error_direct.append(calc_squared_error(transformed_contour, direct_prediction, reference_nodes))\n",
    "    squared_error_grid.append(calc_squared_error(transformed_contour, grid_prediction, reference_nodes))\n",
    "\n",
    "gmsh.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_error_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_and_worst_error_new(error_list):\n",
    "    sum_e_worst = 0  # sum of the largest errors of each mesh\n",
    "    sum_e_mean = 0  # sum of the mse of each mesh\n",
    "    outside = 0   \n",
    "    for mesh in error_list:\n",
    "        # Negative values means predicted point was outside contour. Count it and make it positive.\n",
    "        for i, point_error in enumerate(mesh):\n",
    "            if point_error < 0:\n",
    "                outside += 1\n",
    "                mesh[i] = -point_error\n",
    "        sum_e_worst += max(mesh)\n",
    "        sum_e_mean += sum(mesh)/len(mesh)\n",
    "\n",
    "    e_worst = sum_e_worst/len(error_list)\n",
    "    e_mean = sum_e_mean/len(error_list)\n",
    "    \n",
    "    return [round(e_mean, 3), round(e_worst, 3), outside/len(error_list)]\n",
    "\n",
    "direct_prediction_stats = find_mean_and_worst_error_new(squared_error_direct)\n",
    "grid_prediction_stats = find_mean_and_worst_error_new(squared_error_grid)\n",
    "direct_prediction_stats, grid_prediction_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_mean_and_worst_error(error_list):\n",
    "    total_e_mean = 0\n",
    "    total_e_worst = 0\n",
    "    outside = 0   \n",
    "    for mesh in error_list:\n",
    "        point_outside = False\n",
    "        # -1 means predicted point was outside contour. we do not want to gather stats on that\n",
    "        for point in mesh:\n",
    "            if point == -1:\n",
    "                outside += 1\n",
    "                point_outside = True\n",
    "        if not point_outside:\n",
    "            total_e_worst += max(mesh)\n",
    "            total_e_mean += sum(mesh)/2\n",
    "\n",
    "    e_mean = total_e_mean/len(error_list)\n",
    "    e_worst = total_e_worst/len(error_list)\n",
    "    \n",
    "    return [round(e_mean, 3), round(e_worst, 3), outside/len(error_list)]\n",
    "\n",
    "direct_prediction_stats = find_mean_and_worst_error(squared_error_direct)\n",
    "grid_prediction_stats = find_mean_and_worst_error(squared_error_grid)\n",
    "direct_prediction_stats, grid_prediction_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf"
  },
  "kernelspec": {
   "name": "python374jvsc74a57bd0ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf",
   "display_name": "Python 3.7.4 64-bit ('masterMLvenv': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "5206f4284a6f5b0c767212dafd38c1f4877780ac1730cf1f3a311f04c6a7f3fd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}