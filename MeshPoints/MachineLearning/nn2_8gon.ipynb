{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0ccc2fee2d5adce89579dcecef6a2a73d27aad38392c686d1718faba1504ffcdf",
   "display_name": "Python 3.7.4 64-bit ('masterMLvenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.path as mpl_path\n",
    "import matplotlib.pyplot as plt\n",
    "import pre_processing as pp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from point_grid import *"
   ]
  },
  {
   "source": [
    "# Patch dataset gen - GH data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_count = 3\n",
    "edge_count = 8\n",
    "avg_quality = 0.67\n",
    "\n",
    "df = pd.read_csv('data/8-gon-gh-dataset.csv')\n",
    "# df = df[df.avgQuality > 0.65].dropna(axis=1, how='all')\n",
    "df = df[df.avgQuality > avg_quality]\n",
    "good_data = pd.isnull(df['dum1'])\n",
    "\n",
    "df = df[good_data].dropna(axis=1, how='all')\n",
    "dataset = df.drop(columns = 'avgQuality')\n",
    "\n",
    "\n",
    "new_csv_path = \"data/8-gon-patch-gh-3-internal.csv\"\n",
    "generate_patch_collection_and_write_to_csv(dataset, new_csv_path, edge_count=edge_count,internal_count=internal_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/8-gon-gh-dataset.csv')\n",
    "\n",
    "good_data = pd.isnull(df['dum1'])\n",
    "df = df[good_data].dropna(axis=1, how='all')\n",
    "df.to_csv('data/sanitized-gh-8gon.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================\n",
    "#     Visualization\n",
    "#==========================\n",
    "max_row = dataset.iloc[2373].to_numpy()\n",
    "mr_b = max_row[:-6]\n",
    "mr_i = max_row[-6:]\n",
    "\n",
    "bx, by = mr_b[::2], mr_b[1::2]\n",
    "ix, iy = mr_i[::2], mr_i[1::2]\n",
    "\n",
    "plt.plot(bx, by, 'k')\n",
    "plt.scatter(ix, iy, c='r', marker='x')\n"
   ]
  },
  {
   "source": [
    "# Patch dataset generation - gmsh data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_count = 3\n",
    "edge_count = 8\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/8-gon-correct-procrustes.csv')\n",
    "df = df[df.internal_nodes == 3.0].dropna(axis=1, how='all')\n",
    "df = df[df.target_edge_length == 0.4]\n",
    "dataset = df.drop(['target_edge_length', 'internal_nodes'], axis=1)\n",
    "\n",
    "# Csv-file to write to.\n",
    "new_csv_path = \"data/8-gon-patch-data-gmsh-3-internal.csv\"\n",
    "generate_patch_collection_and_write_to_csv(dataset, new_csv_path, edge_count=edge_count,internal_count=internal_count)"
   ]
  },
  {
   "source": [
    "# Neural network 2: predicting internal node positions (or rather: predict grid score)\n",
    "\n",
    "- 8 edges\n",
    "- 3 internal nodes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "#        DATASET \n",
    "# ======================\n",
    "import pandas as pd\n",
    "\n",
    "patch_path = 'data/8-gon-patch-gh-3-internal.csv'\n",
    "dataset = pd.read_csv(patch_path)\n",
    "\n",
    "# Split dataset into 70/15/15 training/validation/test\n",
    "patch_train = dataset.sample(frac=0.85)\n",
    "patch_test = dataset.drop(patch_train.index)\n",
    "\n",
    "# Split dataset into features and labels; last 4 (grid scores)\n",
    "train_features = patch_train.iloc[:, :-4]\n",
    "train_labels = patch_train.iloc[:, -4:]\n",
    "\n",
    "test_features = patch_test.iloc[:, :-4]\n",
    "test_labels = patch_test.iloc[:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# ======================\n",
    "#       MODEL STUFF\n",
    "# ======================\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "model_path = 'model/thesis-grid-8gon-3int-gh'\n",
    "\n",
    "def grid_model_setup(edge_count=8):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(edge_count*2+8,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(edge_count*2+4, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(edge_count*2+4, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(4),\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = grid_model_setup()\n",
    "model.summary()\n",
    "\n",
    "decay_steps= 10_000\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(1e-1, decay_steps, 1e-3)\n",
    "\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              optimizer=tf.optimizers.Adam(\n",
    "                  learning_rate=lr_schedule,\n",
    "              ),\n",
    "              )\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=EPOCHS//10, min_delta=0.0001)\n",
    "\n",
    "\n",
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.18,\n",
    "                    verbose=2,\n",
    "                    callbacks=[checkpoint, early_stopping],\n",
    "                    )\n",
    "\n",
    "# ======================\n",
    "#       EVALUATION\n",
    "# ======================\n",
    "train_acc = model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc = model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "# plt.ylim(0.15, 0.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim(0.15, 0.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "#  EVALUATE SAVED MODEL \n",
    "# ======================\n",
    "\n",
    "saved_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "train_acc = saved_model.evaluate(\n",
    "    train_features, train_labels, verbose=0)\n",
    "test_acc = saved_model.evaluate(\n",
    "    test_features, test_labels, verbose=0)\n",
    "print('Training data loss: %.3f, Test data loss: %.3f' %\n",
    "        (train_acc, test_acc))"
   ]
  },
  {
   "source": [
    "# Stat gathering for nn2\n",
    "\n",
    "1. Create random contour.\n",
    "2. Find internal mesh points with gmsh, grid nn and direct nn.\n",
    "3. Calculate how often grid/direct is outside, and the mean euclidean error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing method for nn2. \n",
    "def stat_grid_prediction_pipeline(contour, internal_node_count, grid_model):\n",
    "\n",
    "    # Build point grid\n",
    "    point_grid = generate_point_grid()\n",
    "    patches = generate_patches(point_grid)\n",
    "\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour'] \n",
    "    transformed_contour_data = [coordinate for point in transformed_contour for coordinate in point]\n",
    "\n",
    "    # Get patch data\n",
    "    for patch in patches:\n",
    "        patch_data = [coordinate for point in patch for coordinate in point.get_coordinates()]\n",
    "\n",
    "        # Define prediction data\n",
    "        features = np.append(transformed_contour_data, patch_data)\n",
    "        prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        prediction_result = grid_model(prediction_data).numpy()\n",
    "\n",
    "        # Write predicted score to the point grid\n",
    "        for i, p in enumerate(patch):\n",
    "            p.score = prediction_result[0][i] \n",
    "\n",
    "    return point_grid\n",
    "\n",
    "def stat_direct_prediction_pipeline(contour_input_2, direct_model):\n",
    "    # Pre-process contour with procrustes superimposition\n",
    "    transformed_contour_data = [coordinate for point in contour_input_2 for coordinate in point]\n",
    "\n",
    "    # Load model\n",
    "    \n",
    "\n",
    "    # Define prediction data\n",
    "    features = transformed_contour_data\n",
    "    direct_prediction_data = np.expand_dims(features, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    direct_prediction_result = direct_model(direct_prediction_data).numpy()\n",
    "\n",
    "    return direct_prediction_result\n",
    "\n",
    "def calc_dist_error(contour, predicted: list, reference: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates the euclidean distance error of predictions. Assigns a negative value if point is outside of the contour.\n",
    "    Calculates the distance to the _closest_point.\n",
    "    \"\"\"\n",
    "    se_list = []\n",
    "    # Define contour (for finding points inside)\n",
    "    contour_path = mpl_path.Path(contour)\n",
    "\n",
    "    for p_point in predicted:\n",
    "        error = 100\n",
    "        for r_point in reference:\n",
    "            tmp_squared_error = sqrt((p_point[0] - r_point[0])**2 + (p_point[1] - r_point[1])**2)\n",
    "            if tmp_squared_error < error:\n",
    "                error = tmp_squared_error\n",
    "        if contour_path.contains_point(p_point):\n",
    "            se_list.append(error)\n",
    "        else:\n",
    "            se_list.append(-error)\n",
    "    return se_list\n",
    "\n",
    "\n",
    "def find_mean_and_worst_error_new(error_list):\n",
    "    sum_e_worst = 0  # sum of the largest errors of each mesh\n",
    "    sum_e_mean = 0  # sum of the mse of each mesh\n",
    "    outside = 0   \n",
    "    for mesh in error_list:\n",
    "        # Negative values means predicted point was outside contour. Count it and make it positive.\n",
    "        for i, point_error in enumerate(mesh):\n",
    "            if point_error < 0:\n",
    "                outside += 1\n",
    "                mesh[i] = -point_error\n",
    "        sum_e_worst += max(mesh)\n",
    "        sum_e_mean += sum(mesh)/len(mesh)\n",
    "\n",
    "    e_worst = sum_e_worst/len(error_list)\n",
    "    e_mean = sum_e_mean/len(error_list)\n",
    "    \n",
    "    return [round(e_mean, 3), round(e_worst, 3), outside/len(error_list)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import gmsh\n",
    "import pre_processing as pp\n",
    "from point_grid import *\n",
    "\n",
    "# Initialization\n",
    "target_internal_node_count = 3\n",
    "edge_count = 8\n",
    "sample_size = 1000\n",
    "distance_error_grid = []\n",
    "distance_error_direct = []\n",
    "\n",
    "grid_model_path = 'model/thesis-grid-8gon-3int'\n",
    "# direct_model_path = 'model/direct-internal-nodes-correct'\n",
    "grid_model = tf.keras.models.load_model(grid_model_path)\n",
    "# direct_model = tf.keras.models.load_model(direct_model_path)\n",
    "\n",
    "gmsh.initialize()\n",
    "\n",
    "for i in range(sample_size):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Gathering stats on grid model: {grid_model_path} and direct model: {direct_model_path}\")\n",
    "    print(\"\\r\", f\"processing contour {i+1} of {sample_size}\", end=\"\")\n",
    "\n",
    "    # We only check contours that the reference mesher actually place two points inside of.\n",
    "    # Target edge length: 0.4\n",
    "    meshed_internal_node_count = -1\n",
    "    \n",
    "    while meshed_internal_node_count != target_internal_node_count:\n",
    "        contour = pp.create_random_ngon(edge_count)\n",
    "        transformed_contour = pp.procrustes(contour)['transformed_contour']\n",
    "\n",
    "        meshed_contour = pp.mesh_contour(transformed_contour, 0.4)\n",
    "        meshed_internal_points = meshed_contour[-target_internal_node_count*2:]\n",
    "        meshed_internal_node_count = int(meshed_contour[2*edge_count+1])\n",
    "    \n",
    "\n",
    "    # Predict the point grid and intrapolate nodes from it.\n",
    "    predicted_point_grid = stat_grid_prediction_pipeline(transformed_contour, target_internal_node_count, grid_model)\n",
    "    grid_prediction = generate_internal_nodes_from_grid_score(predicted_point_grid, target_internal_node_count)\n",
    "\n",
    "    # Direct prediction without point grid\n",
    "    # direct_prediction_raw = stat_direct_prediction_pipeline(contour, direct_model)[0]\n",
    "    # direct_prediction = list(zip(direct_prediction_raw[::2], direct_prediction_raw[1::2]))\n",
    "    \n",
    "    reference_nodes = list(zip(meshed_internal_points[::2], meshed_internal_points[1::2]))\n",
    "\n",
    "    distance_error_grid.append(calc_dist_error(transformed_contour, grid_prediction, reference_nodes))\n",
    "    # distance_error_direct.append(calc_dist_error(transformed_contour, direct_prediction, reference_nodes))\n",
    "\n",
    "gmsh.finalize()\n",
    "\n",
    "grid_prediction_stats = find_mean_and_worst_error_new(distance_error_grid)\n",
    "# direct_prediction_stats = find_mean_and_worst_error_new(distance_error_direct)\n",
    "grid_prediction_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_prediction_stats, direct_prediction_stats"
   ]
  },
  {
   "source": [
    "# Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmsh\n",
    "\n",
    "grid_model_path = 'auto-model/nn2-8gon-2int'\n",
    "gmsh.initialize()\n",
    "grid_model = tf.keras.models.load_model(grid_model_path)\n",
    "\n",
    "# Naive way of only checking the results on contours that the reference mesher would insert two points into\n",
    "meshed_internal_node_count = -1\n",
    "target_internal_node_count = 2\n",
    "edge_count = 8\n",
    "while meshed_internal_node_count != target_internal_node_count:\n",
    "    contour = pp.create_random_ngon(edge_count)\n",
    "    transformed_contour = pp.procrustes(contour)['transformed_contour']\n",
    "\n",
    "    meshed_contour = pp.mesh_contour(transformed_contour, 0.4)\n",
    "    meshed_internal_points = meshed_contour[-target_internal_node_count*2:]\n",
    "    meshed_internal_node_count = int(meshed_contour[2*edge_count])\n",
    "\n",
    "gmsh.finalize()\n",
    "\n",
    "reference_nodes = list(zip(meshed_internal_points[::2], meshed_internal_points[1::2]))\n",
    "\n",
    "# Predict the point grid and intrapolate nodes from it.\n",
    "predicted_point_grid = stat_prediction_pipeline_nn2(transformed_contour, target_internal_node_count, grid_model)\n",
    "grid_prediction = generate_internal_nodes_from_grid_score(predicted_point_grid, target_internal_node_count)\n",
    "\n",
    "\"\"\"\n",
    "|-------------------------\n",
    "|       PLOTTING\n",
    "|-------------------------\n",
    "\"\"\"\n",
    "flat_pgp = [point for row in predicted_point_grid for point in row]\n",
    "\n",
    "pgp_x = [p.x for p in flat_pgp]\n",
    "pgp_y = [p.y for p in flat_pgp]\n",
    "pgp_s = [p.score for p in flat_pgp]\n",
    "\n",
    "# Plot predicted points (green)\n",
    "pl0 = plt.plot([x[0] for x in reference_nodes], [y[1] for y in reference_nodes], 'gD',\n",
    "            label=\"Reference\",\n",
    "            )\n",
    "pl1 = plt.plot([x[0] for x in grid_prediction], [y[1] for y in grid_prediction], 'rx', label=\"Patch prediction\")\n",
    "# Plot transformed contour\n",
    "pl3 = pp.plot_polygon(transformed_contour, style='k')\n",
    "\n",
    "# Plot point grid\n",
    "pl4 = plt.scatter(pgp_x, pgp_y, c=pgp_s, cmap=plt.cm.Greens, marker='.')\n",
    "plt.colorbar(label=\"Grid point score\")\n",
    "plt.legend(loc='upper right', shadow='true')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\", rotation=0)\n",
    "plt.gca().set_aspect('equal')\n",
    "reference_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}